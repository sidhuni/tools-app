apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgresql-s3-backup
  namespace: postgresql
spec:
  schedule: "*/1 * * * *" # Every minute for local testing
  timeZone: "Asia/Kolkata" # Requires K8s 1.27+
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup-worker
            image: public.ecr.aws/docker/library/ubuntu:24.04
            env:
              - name: TZ
                value: "Asia/Kolkata"
              - name: PGUSER
                value: "admin"
              - name: PGPASSWORD
                valueFrom:
                  secretKeyRef:
                    name: postgresql-secrets
                    key: user_password
              - name: PGHOST
                value: "postgresql" # Your DB Service name
              - name: BUCKET
                value: "postgresqlb-backup"
            command:
              - /bin/sh
              - -c
              - |
                set -e
                
                # 1. Use the official helper to add the Postgres 18 repository
                # This avoids manual GPG folder and path errors.
                apt-get update && apt-get install -y curl ca-certificates postgresql-common
                yes | /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
                
                # 2. Install the Client (Notice version 18 to match your server)
                apt-get update && apt-get install -y postgresql-client-18 unzip gzip
                
                # 3. Use the RELIABLE ARM64 download link for AWS CLI
                # We use -L (follow redirects) and -k (ignore SSL if network is unstable)
                echo "Downloading AWS CLI for ARM64..."
                curl -Lk "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" -o "awscliv2.zip"
                #curl -L "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip -- for Prod 
                
                # Verify file size before unzipping to avoid 'End-of-central-directory' error
                if [ ! -s awscliv2.zip ]; then echo "Download failed: File is empty"; exit 1; fi
                
                unzip -q awscliv2.zip && ./aws/install
                
                # 4. Final verification
                echo "--- Tool Versions ---"
                aws --version
                pg_dump --version

                # 5. Backup Logic
                TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S-%Z)
                DB_LIST=$(psql -h $PGHOST -U "$PGUSER" -d postgres -At -c "SELECT datname FROM pg_database WHERE datistemplate = false AND datname != 'postgres';")

                for DB in $DB_LIST; do
                  echo "Backing up: $DB"
             
                  echo "âœ… Uploaded $DB"
                done

                echo "--- Starting Retention Cleanup (Target: 5 days) ---"

                # Get folders, sort them by name (date), and ensure we only get the directory names
                # 'aws s3 ls' output looks like: PRE 2026-02-11/
                BACKUPS=$(aws s3 ls "s3://$BUCKET/" | grep "PRE " | awk '{print $2}' | sort)

                COUNT=$(echo "$BACKUPS" | grep -c / || true)

                if [ "$COUNT" -gt 5 ]; then
                    # Calculate how many need to be deleted
                    # Example: If COUNT is 6, REMOVE_COUNT is 1
                    REMOVE_COUNT=$((COUNT - 5))
                    
                    # Take the oldest ones (top of the sorted list)
                    REMOVE_LIST=$(echo "$BACKUPS" | head -n "$REMOVE_COUNT")
                    
                    echo "Found $COUNT backup folders. Deleting the oldest $REMOVE_COUNT folder(s)."
                    
                    for OLD_FOLDER in $REMOVE_LIST; do
                        echo "Permanently deleting oldest backup: $OLD_FOLDER"
                        aws s3 rm "s3://$BUCKET/$OLD_FOLDER" --recursive
                    done
                else
                    echo "Only $COUNT backups found. No deletion needed today."
                fi
