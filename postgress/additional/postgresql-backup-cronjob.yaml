apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgresql-s3-backup
  namespace: postgresql
spec:
  schedule: "*/22 * * * *" # Every minute for local testing
  timeZone: "Asia/Kolkata" # Requires K8s 1.27+
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup-worker
            image: public.ecr.aws/docker/library/ubuntu:24.04
            env:
              - name: TZ
                value: "Asia/Kolkata"
              - name: PGUSER
                value: "admin"
              - name: PGPASSWORD
                valueFrom:
                  secretKeyRef:
                    name: postgresql-secrets
                    key: user_password
              - name: PGHOST
                value: "postgresql" # Your DB Service name
              - name: BUCKET
                value: "postgresqlb-backup"
            command:
              - /bin/sh
              - -c
              - |
                set -e
                
                # 1. Use the official helper to add the Postgres 18 repository
                # This avoids manual GPG folder and path errors.
                apt-get update && apt-get install -y curl ca-certificates postgresql-common
                yes | /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
                
                # 2. Install the Client (Notice version 18 to match your server)
                apt-get update && apt-get install -y postgresql-client-18 unzip gzip
                
                # 3. Use the RELIABLE ARM64 download link for AWS CLI
                # We use -L (follow redirects) and -k (ignore SSL if network is unstable)
                echo "Downloading AWS CLI for ARM64..."
                curl -Lk "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" -o "awscliv2.zip"
                #curl -L "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip -- for Prod 
                
                # Verify file size before unzipping to avoid 'End-of-central-directory' error
                if [ ! -s awscliv2.zip ]; then echo "Download failed: File is empty"; exit 1; fi
                
                unzip -q awscliv2.zip && ./aws/install
                
                # 4. Final verification
                echo "--- Tool Versions ---"
                aws --version
                pg_dump --version

                # 5. Backup Logic
                TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S-%Z)
                DB_LIST=$(psql -h $PGHOST -U "$PGUSER" -d postgres -At -c "SELECT datname FROM pg_database WHERE datistemplate = false AND datname != 'postgres';")

                for DB in $DB_LIST; do
                  echo "Backing up: $DB"
             
                  echo "âœ… Uploaded $DB"
                done

                echo "--- Starting Retention Cleanup (Target: 5 days) ---"

                # Get folders, sort them by name (date), and ensure we only get the directory names
                # 'aws s3 ls' output looks like: PRE 2026-02-11/
                BACKUPS=$(aws s3 ls "s3://$BUCKET/" | grep "PRE " | awk '{print $2}' | sort)

                COUNT=$(echo "$BACKUPS" | grep -c / || true)

                if [ "$COUNT" -gt 5 ]; then
                    # Calculate how many need to be deleted
                    # Example: If COUNT is 6, REMOVE_COUNT is 1
                    REMOVE_COUNT=$((COUNT - 5))
                    
                    # Take the oldest ones (top of the sorted list)
                    REMOVE_LIST=$(echo "$BACKUPS" | head -n "$REMOVE_COUNT")
                    
                    echo "Found $COUNT backup folders. Deleting the oldest $REMOVE_COUNT folder(s)."
                    
                    for OLD_FOLDER in $REMOVE_LIST; do
                        echo "Permanently deleting oldest backup: $OLD_FOLDER"
                        aws s3 rm "s3://$BUCKET/$OLD_FOLDER" --recursive
                    done
                else
                    echo "Only $COUNT backups found. No deletion needed today."
                fi

                echo "--- Starting Retention Cleanup (Keep last 5 days) ---"

                # 1. Get all folder names (e.g., 2026-02-11_22-00-59-IST/)
                BACKUPS=$(aws s3 ls "s3://$BUCKET/" | grep "PRE " | awk '{print $2}' | sort)

                # 2. Extract unique dates (YYYY-MM-DD)
                DATES=$(echo "$BACKUPS" | cut -c 1-10 | sort | uniq)

                # 3. Count unique dates
                DATE_COUNT=$(echo "$DATES" | grep -c . || echo 0)

                if [ "$DATE_COUNT" -gt 5 ]; then
                    REMOVE_COUNT=$((DATE_COUNT - 5))
                    REMOVE_DATES=$(echo "$DATES" | head -n "$REMOVE_COUNT")
                    
                    echo "Found $DATE_COUNT unique dates. Deleting folders for the oldest $REMOVE_COUNT date(s):"
                    
                    for OLD_DATE in $REMOVE_DATES; do
                        echo "Searching for folders matching date: $OLD_DATE"
                        
                        # We look for any folder in our BACKUPS list that starts with that date
                        echo "$BACKUPS" | grep "^$OLD_DATE" | while read -r OLD_FOLDER; do
                            # Remove any trailing whitespace/slashes for safety, then execute
                            TARGET=$(echo "$OLD_FOLDER" | xargs)
                            if [ -n "$TARGET" ]; then
                                echo "Permanently deleting: $TARGET"
                                aws s3 rm "s3://$BUCKET/$TARGET" --recursive --dryrun
                            fi
                        done
                    done
                else
                    echo "Only $DATE_COUNT unique backup dates found ($DATE_COUNT/5). No deletion needed."
                fi
